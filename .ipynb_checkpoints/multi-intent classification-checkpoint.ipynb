{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# follow top-to-bottom cell execution.\n",
    "1. import packages\n",
    "2. execute utils \n",
    "2. there are 4 domains data used here. choose any one from next 4-cells & execute.(Toxic/custom/ice-xd(testAPSSDC)/snips)\n",
    "3. execute training cell\n",
    "4. in prediction cell, validation data for each domains data are commented by triple quotes. based on training, choose one.\n",
    "\n",
    "# pros :\n",
    "1. best when query variation limited. search always narrows down to specific domain. example : chatbot used in retail, or other domain.\n",
    "2. performance faster\n",
    "3. memory consumption less\n",
    "\n",
    "# cons :\n",
    "1. can't handle new words like synonym & other form of words.\n",
    "2. as it works on TF/IDF , it'll give more confidence to label which has more TF/TDF.\n",
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle competition : Toxic Comment Classification Challenge\n",
    "https://www.kaggle.com/rhodiumbeng/classifying-multi-label-comments-0-9741-lb/data  -- get data into data directory or change below path as per you convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample\n",
      "                      id                                       comment_text  \\\n",
      "105173  32a5fb2f318ea716  \"\\n\\n Suntag's RfA \\n\\nCould you please elabor...   \n",
      "141992  f78f2377112245fe  Um.... yeah, I know it is.  Go fuck yourself, ...   \n",
      "106660  3a7a5dedbb9dc5f6  Chasing each other \\n\\nI guess you're doing th...   \n",
      "46      001c419c445b5a59  You had a point, and it's now ammended with ap...   \n",
      "147402  3bddfc9d94e0b68f  There is no longer a picture because the wikin...   \n",
      "\n",
      "        toxic  severe_toxic  obscene threat insult identity_hate  \n",
      "105173     NA            NA       NA     NA     NA            NA  \n",
      "141992  toxic  severe_toxic  obscene     NA     NA            NA  \n",
      "106660     NA            NA       NA     NA     NA            NA  \n",
      "46         NA            NA       NA     NA     NA            NA  \n",
      "147402     NA            NA       NA     NA     NA            NA  \n",
      "['obscene', 'insult', 'toxic', 'severe_toxic', 'identity_hate', 'threat']\n",
      "[u'explan edit made usernam hardcor metallica fan revert vandal closur gas vote new york doll fac pleas remov templat talk page sinc retir now'\n",
      " u'aww match background colour seem stuck thank talk januari utc'\n",
      " u'hey man realli tri edit war guy constant remov relev inform talk edit instead talk page seem care format actual info'\n",
      " u'cannot make real suggest improv wonder section statist later subsect type accid think refer need tidi exact format ie date format etc later els first prefer format style refer want pleas let know appear backlog articl review guess delay review turn list relev form eg wikipedia good articl nomin transport'\n",
      " u'sir hero chanc rememb page on']\n",
      "[[], [], [], [], []]\n",
      "[u'thank understand think high would revert without discuss'\n",
      " u'dear god site horribl'\n",
      " u'somebodi invari tri add religion realli mean way peopl invari kept ad religion samuel beckett infobox bother bring long dead complet non exist influenc issu flail make crap fli comparison explicit acknowledg entir amo oz articl person jewish categori'\n",
      " u'say right type type institut need case level suni school univers center doctor grant institut state colleg communiti colleg need case clarifi ub suni center say even binghamton univers univers albani state univers new york stoni brook univers stop tri say total right case'\n",
      " u'ad new product list make sure relev ad new product list make sure wikipedia entri alreadi prove relev give reader possibl read otherwis could subject delet see articl revis histori']\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_labels_df = pd.read_csv('data/test_labels.csv')\n",
    "\n",
    "#print train_df.sample(5)\n",
    "#print test_df.sample(5)\n",
    "test_labels_df = test_labels_df[test_labels_df.toxic != -1]\n",
    "#print len(test_df)\n",
    "test_df = test_df[test_df['id'].isin(test_labels_df.id)]\n",
    "#print len(test_df), len(test_labels_df)\n",
    "#print test_labels_df.sample(5)\n",
    "\n",
    "cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']\n",
    "\n",
    "# special case\n",
    "for col in cols_target:\n",
    "    train_df[col] = train_df[col].map(lambda com : col if com==1 else 'NA')\n",
    "    test_labels_df[col] = test_labels_df[col].map(lambda com : col if com==1 else 'NA')\n",
    "\n",
    "print \"train sample\"\n",
    "print train_df.sample(5)\n",
    "\n",
    "train_labels_df = train_df[cols_target]\n",
    "# Let's look at the character length for the rows in the training data and record these\n",
    "#train_df['char_length'] = train_df['comment_text'].apply(lambda x: len(str(x)))\n",
    "#test_df['char_length'] = test_df['comment_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# explicit check\n",
    "xx = train_df[(train_df['toxic']=='toxic') & (train_df['obscene']=='obscene')].head(5)\n",
    "#print xx\n",
    "#print list(xx['id'])\n",
    "#print list(xx['comment_text'])\n",
    "\n",
    "# clean the comment_text in train_df [Thanks to Pulkit Jha for the useful pointer.]\n",
    "train_df['comment_text'] = train_df['comment_text'].map(lambda com : clean_text(com))\n",
    "train_df['comment_text'] = train_df['comment_text'].str.lower()\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(cleanHtml)\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(cleanPunc)\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(keepAlpha)\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(removeStopWords)\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(stemming)\n",
    "\n",
    "# clean the comment_text in test_df [Thanks, Pulkit Jha.]\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda com : clean_text(com))\n",
    "test_df['comment_text'] = test_df['comment_text'].str.lower()\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(cleanHtml)\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(cleanPunc)\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(keepAlpha)\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(removeStopWords)\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(stemming)\n",
    "#train_df = train_df.drop('char_length',axis=1)\n",
    "\n",
    "train_X = train_df.comment_text\n",
    "test_X = test_df.comment_text\n",
    "\n",
    "target_names = cols_target #target_names\n",
    "X_train = train_X.values #X_train\n",
    "y_train_text = [list(set(i).difference(set([\"NA\"]))) for i in train_labels_df.values.tolist()] #y_train_text\n",
    "X_test = test_X.values #X_test\n",
    "print target_names\n",
    "print X_train[:5]\n",
    "print y_train_text[:5]\n",
    "print X_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom multi-intent datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([\"Hi\",\"Hi what's up?\",\"Hello\",\"new york is a hell of a town\",\n",
    "                    \"new york was originally dutch\",\n",
    "                    \"the big apple is great\",\n",
    "                    \"new york is also called the big apple\",\n",
    "                    \"nyc is nice\",\n",
    "                    \"people abbreviate new york city as nyc\",\n",
    "                    \"the capital of great britain is london\",\n",
    "                    \"london is in the uk\",\n",
    "                    \"london is in england\",\n",
    "                    \"london is in great britain\",\n",
    "                    \"it rains a lot in london\",\n",
    "                    \"london hosts the british museum\",\n",
    "                    \"new york is great and so is london\",\n",
    "                    \"i like london better than new york\",\n",
    "                    \"Hi. london is in the uk\",\n",
    "                    \"hello. nyc is nice\",\"Hi bot.\",\"Hi there.\",\"Hello there.\",\"Hey what's up?\",\n",
    "                    \"Hey.\"])\n",
    "y_train_text = [[\"greet\"],[\"greet\"],[\"greet\"],[\"new york\"],[\"new york\"],[\"new york\"],[\"new york\"],[\"new york\"],\n",
    "                [\"new york\"],[\"london\"],[\"london\"],[\"london\"],[\"london\"],\n",
    "                [\"london\"],[\"london\"],[\"new york\",\"london\"],[\"new york\",\"london\"],\n",
    "                [\"greet\",\"london\"],[\"greet\",\"new york\"],[\"greet\"],[\"greet\"],\n",
    "               [\"greet\"],[\"greet\"],[\"greet\"]]\n",
    "\n",
    "X_test = np.array(['nice day in nyc',\n",
    "                   'welcome to london',\n",
    "                   'london is rainy',\n",
    "                   'is england a london',\n",
    "                   'it is raining in new york and the big apple',\n",
    "                   'it is raining in london and nyc',\n",
    "                   'hello welcome to new york. enjoy it here and london too'\n",
    "                  'hello there. have you been to london'])\n",
    "target_names = ['New York', 'London']\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNIPS intent classification dataset\n",
    "\n",
    "### https://github.com/snipsco/nlu-benchmark     -- clone data into snips_data directory or change below path as per your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "[u'book The Middle East  restaurant in IN for noon', u'Book a table at T-Rex  distant from Halsey St .', u\"I'd like to eat at a taverna that serves chili con carne for a party of 10 \", u'I have a party of four in Japan and need a reservation at Rimsky-Korsakoffee House on Aug. the 3rd .', u'Please make a restaurant reservation for somewhere in Mondovi , Connecticut .'] [u'Add song to my  Club Hits', u'add this tune to my playlist titled uncharted 4 nathan drake', u'Add 100% te ljubam to the Hit Remix playlist', u'Put this album on totally alternative', u\"put this album into becky's  infinite indie folk playlist\"]\n",
      "[['BookRestaurant'], ['BookRestaurant'], ['BookRestaurant'], ['BookRestaurant'], ['BookRestaurant']] [['AddToPlaylist'], ['AddToPlaylist'], ['AddToPlaylist'], ['AddToPlaylist'], ['AddToPlaylist']]\n",
      "500 500\n",
      "Test\n",
      "[u'Book a reservation for my babies and I', u'Book a reservation for a restaurant  not far from MA', u'I would like to book a restaurant in Tanzania that is within walking distance for my mom and I', u'Book a reservation for an oyster bar', u'Book a reservation for 6 people for a creole  tavern in Montenegro'] [u'add we have a theme song to my  House Afterwork playlist', u'add the song to my  We Everywhere playlist', u'Add Roel van Velzen to my  party of the century playlist.', u'Add the artist to the political punks playlist.', u'Add the album to my  Club Hits playlist.']\n",
      "[['BookRestaurant'], ['BookRestaurant'], ['BookRestaurant'], ['BookRestaurant'], ['BookRestaurant']] [['AddToPlaylist'], ['AddToPlaylist'], ['AddToPlaylist'], ['AddToPlaylist'], ['AddToPlaylist']]\n",
      "500 500\n"
     ]
    }
   ],
   "source": [
    "snips_text_train = []\n",
    "snips_labels_train = []\n",
    "snips_text_val = []\n",
    "snips_labels_val = []\n",
    "\n",
    "take_n_samples_per_class = 100\n",
    "\n",
    "import json\n",
    "BR_train = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/BookRestaurant/train_BookRestaurant_full.json\"\n",
    "BR_val = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/BookRestaurant/validate_BookRestaurant.json\"\n",
    "with open(BR_train) as f:\n",
    "    BR_train_data = json.load(f)\n",
    "with open(BR_val) as f:\n",
    "    BR_val_data = json.load(f)\n",
    "\n",
    "text_list_BR = []\n",
    "text_entity_BR = []\n",
    "label_list_BR = []\n",
    "text_list_BR_val = []\n",
    "text_entity_BR_val = []\n",
    "label_list_BR_val = []\n",
    "\n",
    "for each_data in BR_train_data['BookRestaurant'][:take_n_samples_per_class]:\n",
    "    text_list_BR.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_BR.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_BR.append([\"BookRestaurant\"])\n",
    "for each_data in BR_val_data['BookRestaurant'][:500]:\n",
    "    text_list_BR_val.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_BR_val.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_BR_val.append([\"BookRestaurant\"])\n",
    "\n",
    "snips_text_train.extend(text_list_BR)\n",
    "snips_labels_train.extend(label_list_BR)\n",
    "snips_text_val.extend(text_list_BR_val)\n",
    "snips_labels_val.extend(label_list_BR_val)\n",
    "\n",
    "GW_train = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/GetWeather/train_GetWeather_full.json\"\n",
    "GW_val = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/GetWeather/validate_GetWeather.json\"\n",
    "with open(GW_train) as f:\n",
    "    GW_train_data = json.load(f)\n",
    "with open(GW_val) as f:\n",
    "    GW_val_data = json.load(f)\n",
    "\n",
    "text_list_GW = []\n",
    "text_entity_GW = []\n",
    "label_list_GW = []\n",
    "text_list_GW_val = []\n",
    "text_entity_GW_val = []\n",
    "label_list_GW_val = []\n",
    "for each_data in GW_train_data['GetWeather'][:take_n_samples_per_class]:\n",
    "    text_list_GW.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_GW.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_GW.append([\"GetWeather\"])\n",
    "for each_data in GW_val_data['GetWeather'][:500]:\n",
    "    text_list_GW_val.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_GW_val.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_GW_val.append([\"GetWeather\"])\n",
    "\n",
    "snips_text_train.extend(text_list_GW)\n",
    "snips_labels_train.extend(label_list_GW)\n",
    "snips_text_val.extend(text_list_GW_val)\n",
    "snips_labels_val.extend(label_list_GW_val)\n",
    "\n",
    "PM_train = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/PlayMusic/train_PlayMusic_full.json\"\n",
    "PM_val = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/PlayMusic/validate_PlayMusic.json\"\n",
    "with open(PM_train) as f:\n",
    "    PM_train_data = json.load(f)\n",
    "with open(PM_val) as f:\n",
    "    PM_val_data = json.load(f)\n",
    "\n",
    "text_list_PM = []\n",
    "text_entity_PM = []\n",
    "label_list_PM = []\n",
    "text_list_PM_val = []\n",
    "text_entity_PM_val = []\n",
    "label_list_PM_val = []\n",
    "for each_data in PM_train_data['PlayMusic'][:take_n_samples_per_class]:\n",
    "    text_list_PM.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_PM.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_PM.append([\"PlayMusic\"])\n",
    "for each_data in PM_val_data['PlayMusic'][:500]:\n",
    "    text_list_PM_val.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_PM_val.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_PM_val.append([\"PlayMusic\"])\n",
    "\n",
    "snips_text_train.extend(text_list_PM)\n",
    "snips_labels_train.extend(label_list_PM)\n",
    "snips_text_val.extend(text_list_PM_val)\n",
    "snips_labels_val.extend(label_list_PM_val)\n",
    "\n",
    "RB_train = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/RateBook/train_RateBook_full.json\"\n",
    "RB_val = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/RateBook/validate_RateBook.json\"\n",
    "with open(RB_train) as f:\n",
    "    RB_train_data = json.load(f)\n",
    "with open(RB_val) as f:\n",
    "    RB_val_data = json.load(f)\n",
    "\n",
    "text_list_RB = []\n",
    "text_entity_RB = []\n",
    "label_list_RB = []\n",
    "text_list_RB_val = []\n",
    "text_entity_RB_val = []\n",
    "label_list_RB_val = []\n",
    "for each_data in RB_train_data['RateBook'][:take_n_samples_per_class]:\n",
    "    text_list_RB.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_RB.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_RB.append([\"RateBook\"])\n",
    "for each_data in RB_val_data['RateBook'][:500]:\n",
    "    text_list_RB_val.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_RB_val.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_RB_val.append([\"RateBook\"])\n",
    "\n",
    "snips_text_train.extend(text_list_RB)\n",
    "snips_labels_train.extend(label_list_RB)\n",
    "snips_text_val.extend(text_list_RB_val)\n",
    "snips_labels_val.extend(label_list_RB_val)\n",
    "\n",
    "ATP_train = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/AddToPlaylist/train_AddToPlaylist_full.json\"\n",
    "ATP_val = \"snips_data/nlu-benchmark/2017-06-custom-intent-engines/AddToPlaylist/validate_AddToPlaylist.json\"\n",
    "with open(ATP_train) as f:\n",
    "    ATP_train_data = json.load(f)\n",
    "with open(ATP_val) as f:\n",
    "    ATP_val_data = json.load(f)\n",
    "\n",
    "text_list_ATP = []\n",
    "text_entity_ATP = []\n",
    "label_list_ATP = []\n",
    "text_list_ATP_val = []\n",
    "text_entity_ATP_val = []\n",
    "label_list_ATP_val = []\n",
    "for each_data in ATP_train_data['AddToPlaylist'][:take_n_samples_per_class]:\n",
    "    text_list_ATP.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_ATP.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_ATP.append([\"AddToPlaylist\"])\n",
    "for each_data in ATP_val_data['AddToPlaylist'][:500]:\n",
    "    text_list_ATP_val.append(\" \".join([i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    text_entity_ATP_val.append(\" \".join([i.get(\"entity\",\"\") if i.get(\"entity\",None) !=None else i.get(\"text\",\"\") for i in each_data['data']]).replace(\"  \",\" \"))\n",
    "    label_list_ATP_val.append([\"AddToPlaylist\"])\n",
    "\n",
    "snips_text_train.extend(text_list_ATP)\n",
    "snips_labels_train.extend(label_list_ATP)\n",
    "snips_text_val.extend(text_list_ATP_val)\n",
    "snips_labels_val.extend(label_list_ATP_val)\n",
    "\n",
    "print \"Train\"\n",
    "print snips_text_train[:5],snips_text_train[-5:]\n",
    "print snips_labels_train[:5],snips_labels_train[-5:]\n",
    "print len(snips_text_train),len(snips_labels_train)\n",
    "\n",
    "print \"Test\"\n",
    "print snips_text_val[:5],snips_text_val[-5:]\n",
    "print snips_labels_val[:5],snips_labels_val[-5:]\n",
    "print len(snips_text_val),len(snips_labels_val)\n",
    "\n",
    "X_train = np.array(snips_text_train)\n",
    "y_train_text = snips_labels_train\n",
    "target_names = [\"BookRestaurant\",\"GetWeather\",\"PlayMusic\",\"RateBook\",\"AddToPlaylist\"]\n",
    "X_test = np.array(snips_text_val)\n",
    "y_org = snips_labels_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training OneVsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
      " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
      " u'getcoursedetails' u'jobfair_reg_details' u'successflow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "    ..., oob_score=True, random_state=None,\n",
       "            verbose=1, warm_start=False),\n",
       "          n_jobs=1))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_train_text)\n",
    "print mlb.classes_\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, criterion='gini',oob_score=True, class_weight='balanced',n_jobs=-1, verbose=1)\n",
    "\n",
    "#'''\n",
    "# Set the parameters by cross-validation\n",
    "parameters = [{'kernel': ['rbf'],\n",
    "               'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5],\n",
    "                'C': [1, 10, 100, 1000]},\n",
    "              {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "# this is for SGDclassifier\n",
    "'''\n",
    "parameters = {\n",
    "    'penalty': ('l2','l1','elasticnet'),\n",
    "    'loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron','squared_loss', 'huber', 'epsilon_insensitive','squared_epsilon_insensitive'),\n",
    "    'max_iter': (1000,),\n",
    "    'alpha': (0.00001, 0.000001),\n",
    "}\n",
    "'''\n",
    "#print(\"# Tuning hyper-parameters\")\n",
    "#print()\n",
    "\n",
    "#clf = GridSearchCV(svm.SVC(decision_function_shape='ovr'), parameters, cv=5)\n",
    "#clf.fit(X_train, y_train)\n",
    "#'''\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 2),token_pattern=r'\\b\\w+\\b', min_df=1)),#ngram_range=(1, 2),token_pattern=r'\\b\\w+\\b', min_df=1\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    #('clf', OneVsRestClassifier(LinearSVC()))]) # good at ngrams\n",
    "    #('clf', OneVsRestClassifier(CalibratedClassifierCV(SGDClassifier())))])\n",
    "    #('clf', OneVsRestClassifier(CalibratedClassifierCV(GridSearchCV(SGDClassifier(), parameters, cv=5))))])\n",
    "    #('clf', OneVsRestClassifier(CalibratedClassifierCV(svm.SVC(decision_function_shape='ovr'))))])\n",
    "    #('clf', OneVsRestClassifier(CalibratedClassifierCV(GridSearchCV(svm.SVC(decision_function_shape='ovr'), parameters, cv=5))))]) # good at ngrams\n",
    "    #('clf', OneVsRestClassifier(CalibratedClassifierCV(LinearSVC())))]) # good at ngrams\n",
    "    #('clf', OneVsRestClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000,multi_class='ovr', penalty='l2')))])\n",
    "    #('clf', OneVsRestClassifier(LogisticRegression(C=1.,solver='sag'), n_jobs=1))]) #C=1., solver='lbfgs'/'sag'\n",
    "    #('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None)))]) # bad at ngrams\n",
    "    ('clf', OneVsRestClassifier(rf))])\n",
    "\n",
    "classifier.fit(X_train, Y)\n",
    "#classifier.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training OneVsOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
      " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
      " u'getcoursedetails' u'jobfair_reg_details' u'successflow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "    ...mators=500, n_jobs=-1, oob_score=True, random_state=None,\n",
       "            verbose=1, warm_start=False))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_train_text)\n",
    "print mlb.classes_\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, criterion='gini',oob_score=True, class_weight='balanced',n_jobs=-1, verbose=1)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "parameters = [{'kernel': ['rbf'],\n",
    "               'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5],\n",
    "                'C': [1, 10, 100, 1000]},\n",
    "              {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "# this is for SGDclassifier\n",
    "'''\n",
    "parameters = {\n",
    "    'penalty': ('l2','l1','elasticnet'),\n",
    "    'loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron','squared_loss', 'huber', 'epsilon_insensitive','squared_epsilon_insensitive'),\n",
    "    'max_iter': (1000,),\n",
    "    'alpha': (0.00001, 0.000001),\n",
    "}\n",
    "'''\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 2),token_pattern=r'\\b\\w+\\b', min_df=1)),#ngram_range=(1, 2),token_pattern=r'\\b\\w+\\b', min_df=1\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    #('clf', LinearSVC())]) # good at ngrams\n",
    "    #('clf', SGDClassifier())])\n",
    "    #('clf', GridSearchCV(SGDClassifier(), parameters, cv=5))])\n",
    "    #('clf', svm.SVC(decision_function_shape='ovr'))])\n",
    "    #('clf', GridSearchCV(svm.SVC(decision_function_shape='ovr'), parameters, cv=5))]) # good at ngrams\n",
    "    #('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000,multi_class='ovr', penalty='l2'))])\n",
    "    #('clf', LogisticRegression(C=1.,solver='sag'), n_jobs=1)]) #C=1., solver='lbfgs'/'sag'\n",
    "    #('clf', MultinomialNB(fit_prior=True, class_prior=None))]) # bad at ngrams\n",
    "    ('clf',rf)])\n",
    "\n",
    "classifier.fit(X_train, Y)\n",
    "#classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "for category in categories:\n",
    "    print('**Processing {} comments...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, train[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction OneVsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
      " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
      " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict intents index :: [0 0 0 1 0 0 1 0 0 0] \n",
      "\n",
      "predicted intents :: [(u'get_Jobfair_details', u'get_job_details')] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict intents prob :: [[0.    0.004 0.    0.572 0.026 0.    0.534 0.01  0.    0.   ]] \n",
      "\n",
      "first intent 'get_Jobfair_details' with prob 0.572 \n",
      "\n",
      "second intent 'get_job_details' with prob 0.534 \n",
      "\n",
      "third intent 'get_assessment_details' with prob 0.026 \n",
      "\n",
      "predicted intents :: [(u'get_Jobfair_details', u'get_job_details')] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# custom data\n",
    "'''\n",
    "predicted = classifier.predict(X_test)\n",
    "print predicted\n",
    "#print classifier.predict_proba(X_test)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "\n",
    "for item, labels in zip(X_test, all_labels):\n",
    "    print('{0} => {1}'.format(item, ', '.join(labels)))\n",
    "'''\n",
    "\n",
    "# toxic data\n",
    "'''\n",
    "test_label_toxic = [tuple(j for j in i if j!=\"NA\") for i in test_labels_df[cols_target].values.tolist()]\n",
    "pred_true = []\n",
    "for item, org_labels, pred_labels in zip(X_test, test_label_toxic, all_labels):\n",
    "    pred_true.append(all(True if i in org_labels else False for i in pred_labels))\n",
    "    #print('{0} => {1} => {2}'.format(item, ', '.join(org_labels), ', '.join(pred_labels)))\n",
    "print \"Accuracy : \"\n",
    "print pred_true.count(True)/len(pred_true)\n",
    "\n",
    "# sample data predict\n",
    "for comment in xx['comment_text']:\n",
    "    p = np.atleast_1d(np.array([comment,]))\n",
    "    predicted = classifier.predict(p)\n",
    "    print predicted\n",
    "    all_labels = mlb.inverse_transform(predicted)\n",
    "    print all_labels\n",
    "    #print classifier.predict_proba(p)\n",
    "'''\n",
    "\n",
    "#snips data\n",
    "'''\n",
    "predicted = classifier.predict(X_test)\n",
    "#print predicted\n",
    "#print classifier.predict_proba(X_test)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "pred_true = []\n",
    "for item, org_labels, pred_labels in zip(X_test, y_org, all_labels):\n",
    "    #print item,\"***\", org_labels,\"###\", pred_labels\n",
    "    pred_true.append(all(True if i in org_labels else False for i in pred_labels))\n",
    "    #print('{0} => {1} => {2}'.format(item, ', '.join(org_labels), ', '.join(pred_labels)))\n",
    "print \"Accuracy : \"\n",
    "print pred_true.count(True)/len(pred_true)\n",
    "\n",
    "# snips sample data testing for multi-intent\n",
    "sample1 = 'Book a reservation for an oyster bar. with that add the album to my  Club Hits playlist.'\n",
    "sample2 = 'book The Middle East  restaurant in IN for noon and I want to add another album to the wine & dine playlist'\n",
    "p = np.atleast_1d(np.array([sample1,]))\n",
    "predicted = classifier.predict(p)\n",
    "#print predicted\n",
    "#print classifier.predict_proba(X_test)\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "print \"sample class : \",all_labels\n",
    "'''\n",
    "\n",
    "# ice-xd :: testAPSSDC sample data testing for multi-intent\n",
    "sample1 = 'hi, May I know the job fair detail of asasasas and  job openings in dsdsdsdsdsdsd'\n",
    "sample2 = 'Give the grrtrtrt course application status and give me yuy course assessment details'\n",
    "sample3 = 'OK Thanks. can i get your job role specification JOBROLE'\n",
    "sample4 = 'May I know details of job PERSON details any courses available in SECTOR sector ?'\n",
    "#sample_y = [0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0]\n",
    "#for index in [itemindex1[0][0],itemindex2[0][0]]:\n",
    "#    sample_y[index] = 1\n",
    "sample1_y = [0 ,0 ,0 ,1 ,0 ,0 ,1 ,0 ,0 ,0]\n",
    "sample2_y = [0 ,0 ,0 ,0 ,1 ,0 ,0 ,1 ,0 ,0]\n",
    "\n",
    "print \"intents list :: {0} \\n\".format(mlb.classes_)\n",
    "\n",
    "p = np.atleast_1d(np.array([sample1,]))\n",
    "predicted = classifier.predict(p)\n",
    "print \"predict intents index :: {0} \\n\".format(predicted[0])\n",
    "\n",
    "\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "print \"predicted intents :: {} \".format(all_labels)\n",
    "\n",
    "predict_score = classifier.predict_proba(p)\n",
    "print \"predict intents prob :: {0} \\n\".format(predict_score)\n",
    "\n",
    "#'''\n",
    "# get 1,2,3 max when CalibratedClassifierCV or logisticReg used.\n",
    "flat=predict_score.flatten()\n",
    "flat.sort()\n",
    "itemindex1 = np.where(predict_score[0]==flat[-1])\n",
    "print \"first intent '{0}' with prob {1} \\n\".format(mlb.classes_[itemindex1[0][0]],flat[-1])\n",
    "itemindex2 = np.where(predict_score[0]==flat[-2])\n",
    "print \"second intent '{0}' with prob {1} \\n\".format(mlb.classes_[itemindex2[0][0]],flat[-2])\n",
    "itemindex3 = np.where(predict_score[0]==flat[-3])\n",
    "print \"third intent '{0}' with prob {1} \\n\".format(mlb.classes_[itemindex3[0][0]],flat[-3])\n",
    "#'''\n",
    "\n",
    "# test score\n",
    "#print \"score :: {}\".format(classifier.score(p,np.array([sample1_y])))\n",
    "\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "print \"predicted intents :: {} \".format(all_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction OneVsOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
      " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
      " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict intents index :: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "predicted intents :: [()] \n",
      "predict intents prob :: [array([[1., 0.]]), array([[0.994, 0.006]]), array([[1., 0.]]), array([[0.534, 0.466]]), array([[0.988, 0.012]]), array([[1., 0.]]), array([[0.536, 0.464]]), array([[0.948, 0.052]]), array([[1., 0.]]), array([[1., 0.]])] \n",
      "\n",
      "**  predicted intents :: [()] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# custom data\n",
    "'''\n",
    "predicted = classifier.predict(X_test)\n",
    "print predicted\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "\n",
    "for item, labels in zip(X_test, all_labels):\n",
    "    print('{0} => {1}'.format(item, ', '.join(labels)))\n",
    "'''\n",
    "\n",
    "# toxic data\n",
    "'''\n",
    "test_label_toxic = [tuple(j for j in i if j!=\"NA\") for i in test_labels_df[cols_target].values.tolist()]\n",
    "pred_true = []\n",
    "for item, org_labels, pred_labels in zip(X_test, test_label_toxic, all_labels):\n",
    "    pred_true.append(all(True if i in org_labels else False for i in pred_labels))\n",
    "    #print('{0} => {1} => {2}'.format(item, ', '.join(org_labels), ', '.join(pred_labels)))\n",
    "print \"Accuracy : \"\n",
    "print pred_true.count(True)/len(pred_true)\n",
    "\n",
    "# sample data predict\n",
    "for comment in xx['comment_text']:\n",
    "    p = np.atleast_1d(np.array([comment,]))\n",
    "    predicted = classifier.predict(p)\n",
    "    print predicted\n",
    "    all_labels = mlb.inverse_transform(predicted)\n",
    "    print all_labels\n",
    "'''\n",
    "\n",
    "#snips data\n",
    "'''\n",
    "predicted = classifier.predict(X_test)\n",
    "#print predicted\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "pred_true = []\n",
    "for item, org_labels, pred_labels in zip(X_test, y_org, all_labels):\n",
    "    #print item,\"***\", org_labels,\"###\", pred_labels\n",
    "    pred_true.append(all(True if i in org_labels else False for i in pred_labels))\n",
    "    #print('{0} => {1} => {2}'.format(item, ', '.join(org_labels), ', '.join(pred_labels)))\n",
    "print \"Accuracy : \"\n",
    "print pred_true.count(True)/len(pred_true)\n",
    "\n",
    "# snips sample data testing for multi-intent\n",
    "sample1 = 'Book a reservation for an oyster bar. with that add the album to my  Club Hits playlist.'\n",
    "sample2 = 'book The Middle East  restaurant in IN for noon and I want to add another album to the wine & dine playlist'\n",
    "p = np.atleast_1d(np.array([sample1,]))\n",
    "predicted = classifier.predict(p)\n",
    "#print predicted\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "print \"sample class : \",all_labels\n",
    "'''\n",
    "\n",
    "# ice-xd :: testAPSSDC sample data testing for multi-intent\n",
    "sample1 = 'hi, May I know the job fair detail of asasasas and  job openings in dsdsdsdsdsdsd'\n",
    "sample2 = 'Give the grrtrtrt course application status and give me yuy course assessment details'\n",
    "sample3 = 'OK Thanks. can i get your job role specification JOBROLE'\n",
    "sample4 = 'May I know details of job PERSON details any courses available in SECTOR sector ?'\n",
    "#sample_y = [0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0]\n",
    "#for index in [itemindex1[0][0],itemindex2[0][0]]:\n",
    "#    sample_y[index] = 1\n",
    "sample1_y = [0 ,0 ,0 ,1 ,0 ,0 ,1 ,0 ,0 ,0]\n",
    "sample2_y = [0 ,0 ,0 ,0 ,1 ,0 ,0 ,1 ,0 ,0]\n",
    "\n",
    "print \"intents list :: {0} \\n\".format(mlb.classes_)\n",
    "\n",
    "p = np.atleast_1d(np.array([sample1,]))\n",
    "predicted = classifier.predict(p)\n",
    "print \"predict intents index :: {0} \\n\".format(predicted[0])\n",
    "\n",
    "#print help(classifier.predict_proba)\n",
    "\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "print \"predicted intents :: {} \".format(all_labels)\n",
    "\n",
    "try:\n",
    "    predict_score = classifier.predict_proba(p)\n",
    "    print \"predict intents prob :: {0} \\n\".format(predict_score)\n",
    "    print \"** \",predict_score[:,1]\n",
    "    #print np.round(predict_score,2)\n",
    "\n",
    "    #'''\n",
    "    # get 1,2,3 max when CalibratedClassifierCV or logisticReg used.\n",
    "    flat=predict_score.flatten()\n",
    "    flat.sort()\n",
    "    itemindex1 = np.where(predict_score[0]==flat[-1])\n",
    "    print \"first intent '{0}' with prob {1} \\n\".format(mlb.classes_[itemindex1[0][0]],flat[-1])\n",
    "    itemindex2 = np.where(predict_score[0]==flat[-2])\n",
    "    print \"second intent '{0}' with prob {1} \\n\".format(mlb.classes_[itemindex2[0][0]],flat[-2])\n",
    "    itemindex3 = np.where(predict_score[0]==flat[-3])\n",
    "    print \"third intent '{0}' with prob {1} \\n\".format(mlb.classes_[itemindex3[0][0]],flat[-3])\n",
    "    #'''\n",
    "except:\n",
    "    pass\n",
    "\n",
    "all_labels = mlb.inverse_transform(predicted)\n",
    "print \"predicted intents :: {} \".format(all_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# good model . but memory consumption high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f7b081ff4fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Note that this classifier can throw up errors when handling sparse matrices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/achyuta/Achyuta/venv27/local/lib/python2.7/site-packages/scipy/sparse/lil.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/achyuta/Achyuta/venv27/local/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "classifier_new = MLkNN(k=10)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(X_train)\n",
    "vectorizer.fit(X_test)\n",
    "\n",
    "x_train = vectorizer.transform(X_train)\n",
    "\n",
    "#y_train = vectorizer.transform(y_train_text)\n",
    "x_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Note that this classifier can throw up errors when handling sparse matrices.\n",
    "\n",
    "x_train = lil_matrix(x_train).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "x_test = lil_matrix(x_test).toarray()\n",
    "\n",
    "# train\n",
    "classifier_new.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions_new = classifier_new.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions_new))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "\n",
    "# SGDClassifier with gridsearchCV\n",
    "# sample1\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 0 0 0 1 0 0 0] \n",
    "\n",
    "predict intents prob :: [[0.14845367 0.00999279 0.0081006  0.4624159  0.02503919 0.00444298\n",
    "  0.52334891 0.00770991 0.00615307 0.00859134]] \n",
    "\n",
    "first intent 'get_job_details' with prob 0.523348906737 \n",
    "\n",
    "second intent 'get_Jobfair_details' with prob 0.462415899745 \n",
    "\n",
    "third intent 'Greetings' with prob 0.148453672872 \n",
    "\n",
    "predicted intents :: [(u'get_job_details',)] \n",
    "        \n",
    "# sample2\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 0 1 0 0 0 0 0] \n",
    "\n",
    "predict intents prob :: [[0.01248607 0.01102929 0.00662873 0.10694141 0.84651563 0.00699763\n",
    "  0.00548128 0.19659661 0.02024358 0.00762391]] \n",
    "\n",
    "first intent 'get_assessment_details' with prob 0.846515625599 \n",
    "\n",
    "second intent 'getcoursedetails' with prob 0.196596611036 \n",
    "\n",
    "third intent 'get_Jobfair_details' with prob 0.10694141261 \n",
    "\n",
    "predicted intents :: [(u'get_assessment_details',)] \n",
    "\n",
    "# SGD without gridsearchCV\n",
    "# sample1\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 0 0 0 0 0 0 0] \n",
    "\n",
    "predict intents prob :: [[4.71098626e-02 2.03569200e-02 3.01329079e-03 2.96368792e-01\n",
    "  3.33655618e-04 2.03372786e-03 2.51610079e-01 1.00191516e-04\n",
    "  3.11721327e-03 2.25364239e-03]] \n",
    "\n",
    "first intent 'get_Jobfair_details' with prob 0.296368791512 \n",
    "\n",
    "second intent 'get_job_details' with prob 0.251610078825 \n",
    "\n",
    "third intent 'Greetings' with prob 0.0471098625678 \n",
    "\n",
    "predicted intents :: [()] \n",
    "# sample2\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 0 1 0 0 0 0 0] \n",
    "\n",
    "predict intents prob :: [[0.00230899 0.01561245 0.00656202 0.01591573 0.99986673 0.00464843\n",
    "  0.01037208 0.13693212 0.0149582  0.00198682]] \n",
    "\n",
    "first intent 'get_assessment_details' with prob 0.999866729937 \n",
    "\n",
    "second intent 'getcoursedetails' with prob 0.136932118973 \n",
    "\n",
    "third intent 'get_Jobfair_details' with prob 0.0159157289639 \n",
    "\n",
    "predicted intents :: [(u'get_assessment_details',)] \n",
    "\n",
    "# svm.SVC with gridsearchCV\n",
    "# sample1\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 0 0 0 0 0 0 0] \n",
    "\n",
    "predict intents prob :: [[1.68392978e-01 2.82490942e-02 8.51079907e-03 4.96766230e-01\n",
    "  1.85874001e-03 6.52101475e-03 4.61781836e-01 3.95648082e-04\n",
    "  3.98004865e-03 7.27558009e-03]] \n",
    "\n",
    "first intent 'get_Jobfair_details' with prob 0.496766229514 \n",
    "\n",
    "second intent 'get_job_details' with prob 0.461781835736 \n",
    "\n",
    "third intent 'Greetings' with prob 0.1683929782 \n",
    "\n",
    "predicted intents :: [()] \n",
    "# sample2\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 0 1 0 0 0 0 0] \n",
    "\n",
    "predict intents prob :: [[0.00721767 0.02014245 0.0081212  0.01055531 0.99996965 0.0067766\n",
    "  0.0129972  0.13290952 0.00840004 0.00700586]] \n",
    "\n",
    "first intent 'get_assessment_details' with prob 0.99996964881 \n",
    "\n",
    "second intent 'getcoursedetails' with prob 0.132909515458 \n",
    "\n",
    "third intent 'No intent' with prob 0.0201424480183 \n",
    "\n",
    "predicted intents :: [(u'get_assessment_details',)] \n",
    "\n",
    "# svm.SVC without gridSearchCV\n",
    "# sample1\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 1 0 0 1 0 0 0] \n",
    "\n",
    "predict intents prob :: [[0.39198073 0.03528569 0.00725001 0.62577404 0.00397502 0.00422488\n",
    "  0.84447324 0.00186998 0.00605879 0.00674809]] \n",
    "\n",
    "first intent 'get_job_details' with prob 0.844473238473 \n",
    "\n",
    "second intent 'get_Jobfair_details' with prob 0.625774041331 \n",
    "\n",
    "third intent 'Greetings' with prob 0.391980733892 \n",
    "\n",
    "predicted intents :: [(u'get_Jobfair_details', u'get_job_details')] \n",
    "# sample2\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 0 1 0 0 0 0 0] \n",
    "\n",
    "predict intents prob :: [[0.00916612 0.02612597 0.00819797 0.02594459 0.99999559 0.00584121\n",
    "  0.01214022 0.06572587 0.0099974  0.00999234]] \n",
    "\n",
    "first intent 'get_assessment_details' with prob 0.999995586966 \n",
    "\n",
    "second intent 'getcoursedetails' with prob 0.0657258670384 \n",
    "\n",
    "third intent 'No intent' with prob 0.0261259650633 \n",
    "\n",
    "predicted intents :: [(u'get_assessment_details',)] \n",
    "\n",
    "# LinearSVC\n",
    "# sample1\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 1 0 0 0 0 0 0] \n",
    "\n",
    "predict intents prob :: [[2.63027463e-01 2.42802561e-02 7.65685202e-03 5.14094903e-01\n",
    "  1.85059975e-03 5.92744812e-03 4.51604164e-01 3.40920662e-04\n",
    "  7.19157501e-03 6.50554473e-03]] \n",
    "\n",
    "first intent 'get_Jobfair_details' with prob 0.514094902547 \n",
    "\n",
    "second intent 'get_job_details' with prob 0.451604164483 \n",
    "\n",
    "third intent 'Greetings' with prob 0.26302746298 \n",
    "\n",
    "predicted intents :: [(u'get_Jobfair_details',)] \n",
    "# sample2\n",
    "intents list :: [u'Greetings' u'No intent' u'conclusionflow' u'get_Jobfair_details'\n",
    " u'get_assessment_details' u'get_document_details' u'get_job_details'\n",
    " u'getcoursedetails' u'jobfair_reg_details' u'successflow'] \n",
    "\n",
    "predict intents index :: [0 0 0 0 1 0 0 0 0 0] \n",
    "\n",
    "predict intents prob :: [[0.00625314 0.01737233 0.00754981 0.01038664 0.99997775 0.00631261\n",
    "  0.00995603 0.1164047  0.00791785 0.00659245]] \n",
    "\n",
    "first intent 'get_assessment_details' with prob 0.999977750678 \n",
    "\n",
    "second intent 'getcoursedetails' with prob 0.116404696589 \n",
    "\n",
    "third intent 'No intent' with prob 0.0173723341191 \n",
    "\n",
    "predicted intents :: [(u'get_assessment_details',)] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
